{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the results for DS1: \n",
      "\n",
      "The coefficients for the LDA calculation are as follows: \n",
      "\n",
      "The positive prior is 0.497857142857\n",
      "\n",
      "The negative prior is 0.502142857143\n",
      "\n",
      "The positive mean is [ 1.93448969  2.00295145  1.99667886  1.92585977  1.9621975   2.01721409\n",
      "  1.95728451  2.05619103  2.00455781  2.03788155  1.97682959  2.05835065\n",
      "  2.03150181  2.00768923  2.00110638  2.07775191  2.03905353  2.04529506\n",
      "  2.02393877  1.95397961]\n",
      "\n",
      "The negative mean is [ 1.43655335  1.414106    1.39423215  1.43490323  1.38454747  1.41369849\n",
      "  1.3447257   1.40111909  1.36014397  1.41086586  1.35411001  1.41832625\n",
      "  1.48031281  1.41075649  1.37840059  1.36338222  1.40519073  1.4533138\n",
      "  1.38792136  1.44745185]\n",
      "\n",
      "The covariance matrix is [[ 7.82242096  5.38053996  5.98343811  5.02949709  5.72034393  6.03307943\n",
      "   4.5242569   5.33693917  4.84640717  5.0894405   3.82798856  5.11219568\n",
      "   6.88491429  5.87780683  5.99236862  5.85536278  5.69904181  5.62078796\n",
      "   5.46032337  5.76479469]\n",
      " [ 5.38053996  6.8231228   5.22126205  4.22943319  5.39121639  5.43323088\n",
      "   4.26508088  3.85695469  4.10097688  5.02707255  3.33429451  4.55295337\n",
      "   5.81439218  5.04778135  5.40843918  5.20559429  5.54813691  5.12115168\n",
      "   5.28101388  5.23007232]\n",
      " [ 5.98343811  5.22126205  7.14058759  4.68854483  5.64863862  6.42127502\n",
      "   4.47239613  4.64229303  4.71781032  4.89595276  3.15193895  4.4887756\n",
      "   6.14053642  4.99668996  5.98909019  5.80603416  6.05387202  4.85725662\n",
      "   4.55811374  4.86204976]\n",
      " [ 5.02949709  4.22943319  4.68854483  5.58499064  5.14482811  4.41694622\n",
      "   3.5681538   4.24098723  3.26925656  4.16864991  2.65906294  4.01666798\n",
      "   5.64054582  4.67919474  4.71477522  5.01482034  4.51456516  4.50300434\n",
      "   3.75688442  5.60848015]\n",
      " [ 5.72034393  5.39121639  5.64863862  5.14482811  6.97931672  5.29582554\n",
      "   4.9330388   4.27275098  4.60093948  5.11779673  4.02981251  4.91123246\n",
      "   6.08678208  5.79116462  5.95232277  6.1736897   5.81367264  5.16638468\n",
      "   5.29023187  5.6189145 ]\n",
      " [ 6.03307943  5.43323088  6.42127502  4.41694622  5.29582554  6.57745033\n",
      "   4.32630921  4.78341488  4.59939212  5.20749022  2.90625387  4.67727385\n",
      "   6.24186193  4.86526574  5.7146458   5.8769936   5.76407111  4.90442488\n",
      "   4.65528669  5.10603491]\n",
      " [ 4.5242569   4.26508088  4.47239613  3.5681538   4.9330388   4.32630921\n",
      "   5.04842344  3.77762159  3.85188302  4.28608915  2.93561028  4.23831377\n",
      "   4.58571358  4.02072516  4.65172489  5.20519754  4.21422526  3.65719836\n",
      "   4.52012973  3.75827822]\n",
      " [ 5.33693917  3.85695469  4.64229303  4.24098723  4.27275098  4.78341488\n",
      "   3.77762159  5.87928726  3.45905867  4.87690042  2.45985798  4.73731296\n",
      "   5.99510938  5.02024372  4.75319215  5.99642433  4.38366755  4.83688171\n",
      "   4.67294164  4.33452546]\n",
      " [ 4.84640717  4.10097688  4.71781032  3.26925656  4.60093948  4.59939212\n",
      "   3.85188302  3.45905867  4.86151844  4.55329564  3.05289477  3.96851292\n",
      "   4.9502004   4.56706342  4.86393705  5.06178337  5.01056209  4.00853705\n",
      "   4.16031243  3.96362904]\n",
      " [ 5.0894405   5.02707255  4.89595276  4.16864991  5.11779673  5.20749022\n",
      "   4.28608915  4.87690042  4.55329564  7.07833452  3.23428779  4.72312473\n",
      "   6.82191289  5.1890777   4.99869309  5.97903271  4.66818662  5.30038875\n",
      "   4.54953952  5.34614253]\n",
      " [ 3.82798856  3.33429451  3.15193895  2.65906294  4.02981251  2.90625387\n",
      "   2.93561028  2.45985798  3.05289477  3.23428779  3.25070199  2.84919006\n",
      "   3.60787344  3.85445428  3.54763885  3.58007929  3.56163653  3.19271795\n",
      "   3.68254281  3.34058782]\n",
      " [ 5.11219568  4.55295337  4.4887756   4.01666798  4.91123246  4.67727385\n",
      "   4.23831377  4.73731296  3.96851292  4.72312473  2.84919006  5.15017537\n",
      "   5.86776149  4.88716169  4.70946505  5.43931943  4.91096697  4.82754826\n",
      "   4.9307745   4.37636365]\n",
      " [ 6.88491429  5.81439218  6.14053642  5.64054582  6.08678208  6.24186193\n",
      "   4.58571358  5.99510938  4.9502004   6.82191289  3.60787344  5.86776149\n",
      "   8.88626069  6.14080103  6.13562778  6.43098864  5.80307716  6.25579584\n",
      "   5.75530895  7.13392637]\n",
      " [ 5.87780683  5.04778135  4.99668996  4.67919474  5.79116462  4.86526574\n",
      "   4.02072516  5.02024372  4.56706342  5.1890777   3.85445428  4.88716169\n",
      "   6.14080103  6.79252686  5.79265688  6.1367474   5.57952772  5.58788752\n",
      "   5.73605355  5.37458995]\n",
      " [ 5.99236862  5.40843918  5.98909019  4.71477522  5.95232277  5.7146458\n",
      "   4.65172489  4.75319215  4.86393705  4.99869309  3.54763885  4.70946505\n",
      "   6.13562778  5.79265688  6.92401245  7.05747305  5.73936203  5.10147265\n",
      "   5.37523415  5.63817041]\n",
      " [ 5.85536278  5.20559429  5.80603416  5.01482034  6.1736897   5.8769936\n",
      "   5.20519754  5.99642433  5.06178337  5.97903271  3.58007929  5.43931943\n",
      "   6.43098864  6.1367474   7.05747305  9.14387164  5.75413539  5.86124669\n",
      "   5.54657422  5.79391851]\n",
      " [ 5.69904181  5.54813691  6.05387202  4.51456516  5.81367264  5.76407111\n",
      "   4.21422526  4.38366755  5.01056209  4.66818662  3.56163653  4.91096697\n",
      "   5.80307716  5.57952772  5.73936203  5.75413539  6.62963204  5.11527229\n",
      "   5.22120904  4.7591843 ]\n",
      " [ 5.62078796  5.12115168  4.85725662  4.50300434  5.16638468  4.90442488\n",
      "   3.65719836  4.83688171  4.00853705  5.30038875  3.19271795  4.82754826\n",
      "   6.25579584  5.58788752  5.10147265  5.86124669  5.11527229  5.96812145\n",
      "   4.81396636  5.09621488]\n",
      " [ 5.46032337  5.28101388  4.55811374  3.75688442  5.29023187  4.65528669\n",
      "   4.52012973  4.67294164  4.16031243  4.54953952  3.68254281  4.9307745\n",
      "   5.75530895  5.73605355  5.37523415  5.54657422  5.22120904  4.81396636\n",
      "   6.23595478  4.76211437]\n",
      " [ 5.76479469  5.23007232  4.86204976  5.60848015  5.6189145   5.10603491\n",
      "   3.75827822  4.33452546  3.96362904  5.34614253  3.34058782  4.37636365\n",
      "   7.13392637  5.37458995  5.63817041  5.79391851  4.7591843   5.09621488\n",
      "   4.76211437  7.85480759]]\n",
      "\n",
      "The accuracy for LDA is 0.942452043369\n",
      "\n",
      "The precision for LDA is 0.940789473684\n",
      "\n",
      "The recall for LDA is 0.945454545455\n",
      "\n",
      "The F1 score for LDA is 0.943116240725\n",
      "\n",
      "The accuracy for kNN with k value of 1 is 0.518765638032\n",
      "\n",
      "The precision for kNN with k value of 1 is 0.521943573668\n",
      "\n",
      "The recall for kNN with k value of 1 is 0.55041322314\n",
      "\n",
      "The F1 score for kNN with k value of 1 is 0.535800482703\n",
      "\n",
      "The accuracy for kNN with k value of 5 is 0.511259382819\n",
      "\n",
      "The precision for kNN with k value of 5 is 0.514820592824\n",
      "\n",
      "The recall for kNN with k value of 5 is 0.545454545455\n",
      "\n",
      "The F1 score for kNN with k value of 5 is 0.529695024077\n",
      "\n",
      "The accuracy for kNN with k value of 10 is 0.538782318599\n",
      "\n",
      "The precision for kNN with k value of 10 is 0.553497942387\n",
      "\n",
      "The recall for kNN with k value of 10 is 0.444628099174\n",
      "\n",
      "The F1 score for kNN with k value of 10 is 0.493125572869\n",
      "\n",
      "The accuracy for kNN with k value of 15 is 0.554628857381\n",
      "\n",
      "The precision for kNN with k value of 15 is 0.555382215289\n",
      "\n",
      "The recall for kNN with k value of 15 is 0.588429752066\n",
      "\n",
      "The F1 score for kNN with k value of 15 is 0.571428571429\n",
      "\n",
      "The accuracy for kNN with k value of 20 is 0.536280233528\n",
      "\n",
      "The precision for kNN with k value of 20 is 0.545794392523\n",
      "\n",
      "The recall for kNN with k value of 20 is 0.482644628099\n",
      "\n",
      "The F1 score for kNN with k value of 20 is 0.512280701754\n",
      "\n",
      "\n",
      "\n",
      "Here are the results for DS2: \n",
      "\n",
      "The coefficients for the LDA calculation are as follows: \n",
      "\n",
      "The positive prior is 0.504642857143\n",
      "\n",
      "The negative prior is 0.495357142857\n",
      "\n",
      "The positive mean is [ 1.26326143  1.39148481  1.27427822  1.26486856  1.35219702  1.28222596\n",
      "  1.30332012  1.25429299  1.24260068  1.2665678   1.27356604  1.265158\n",
      "  1.31456691  1.24814383  1.31740194  1.29912737  1.35252466  1.24435257\n",
      "  1.29824324  1.28730213]\n",
      "\n",
      "The negative mean is [ 0.93913437  1.06416885  0.97410161  1.06265841  1.02666664  1.07867025\n",
      "  1.03243411  0.96466262  0.94540705  1.02094232  0.99272512  1.02035621\n",
      "  0.98545154  1.03819026  0.97494872  1.05500717  0.94332085  0.99046389\n",
      "  1.01018779  0.95451243]\n",
      "\n",
      "The covariance matrix is [[ 7.7650445   5.52696876  4.65871572  5.02676444  4.33898392  5.69550815\n",
      "   5.93197511  5.53578373  4.61719417  5.24396439  5.63150435  4.92883336\n",
      "   5.04619891  6.04905348  5.28921179  5.75538848  5.34123359  5.38001392\n",
      "   5.61558495  5.95419905]\n",
      " [ 5.52696876  7.25276991  4.85600675  5.22553923  4.99656676  5.97296895\n",
      "   6.51891955  5.35596226  4.57539002  5.11888971  4.96496755  4.99572629\n",
      "   4.90422542  6.00659135  5.60138633  5.97912567  5.1180608   5.036689\n",
      "   6.14664199  5.7425081 ]\n",
      " [ 4.65871572  4.85600675  6.58964169  5.05334833  4.73424944  4.68210928\n",
      "   5.90215369  4.46080409  4.43950276  4.60993519  4.82825584  4.92803514\n",
      "   4.97529869  5.78626953  5.22506549  4.86697288  5.08831112  4.38595062\n",
      "   5.6744452   5.65699153]\n",
      " [ 5.02676444  5.22553923  5.05334833  6.49755123  4.16082499  5.44123263\n",
      "   6.17816736  5.3018764   4.6434817   5.32152235  5.0045327   4.79620217\n",
      "   4.82722483  5.27617474  5.15316218  5.31955995  4.86694351  5.10386795\n",
      "   5.76716533  5.5800473 ]\n",
      " [ 4.33898392  4.99656676  4.73424944  4.16082499  5.65421546  4.63556979\n",
      "   5.06450281  4.37547343  3.67210412  4.00004829  3.76362354  4.12175288\n",
      "   4.21435533  4.69014996  5.14161442  4.88927522  4.1996268   4.41880525\n",
      "   5.19617987  4.90755185]\n",
      " [ 5.69550815  5.97296895  4.68210928  5.44123263  4.63556979  7.72660766\n",
      "   6.59891666  5.18672546  5.41306955  5.40387213  4.86046189  4.85015529\n",
      "   5.23223575  6.22647147  5.5285925   5.87090877  5.30859608  5.32550337\n",
      "   6.4972565   6.10561302]\n",
      " [ 5.93197511  6.51891955  5.90215369  6.17816736  5.06450281  6.59891666\n",
      "   8.37975564  5.9276301   5.70266817  6.03391494  5.99394783  5.73975797\n",
      "   5.55922928  6.79290257  5.91245522  6.28401632  5.63237327  5.57559039\n",
      "   7.20765799  6.37142335]\n",
      " [ 5.53578373  5.35596226  4.46080409  5.3018764   4.37547343  5.18672546\n",
      "   5.9276301   6.32744491  4.51983865  4.94400773  4.57618872  4.65141661\n",
      "   4.19739906  5.23296105  5.09937038  5.26179042  4.61080017  5.47364828\n",
      "   5.44274183  5.31884808]\n",
      " [ 4.61719417  4.57539002  4.43950276  4.6434817   3.67210412  5.41306955\n",
      "   5.70266817  4.51983865  5.78281546  4.40771158  4.27954443  4.32501242\n",
      "   4.24952963  4.90897012  4.60687165  4.45203149  4.45949958  4.52958746\n",
      "   4.849667    4.79341285]\n",
      " [ 5.24396439  5.11888971  4.60993519  5.32152235  4.00004829  5.40387213\n",
      "   6.03391494  4.94400773  4.40771158  6.54365895  4.73567515  4.51968578\n",
      "   4.73717833  5.47801761  4.92648508  5.09645573  4.7073776   4.45144078\n",
      "   6.09721564  5.22444098]\n",
      " [ 5.63150435  4.96496755  4.82825584  5.0045327   3.76362354  4.86046189\n",
      "   5.99394783  4.57618872  4.27954443  4.73567515  6.28512779  5.06031798\n",
      "   4.72004395  5.16829472  5.03436515  4.83030784  4.78027388  4.58695315\n",
      "   5.71676095  5.46454585]\n",
      " [ 4.92883336  4.99572629  4.92803514  4.79620217  4.12175288  4.85015529\n",
      "   5.73975797  4.65141661  4.32501242  4.51968578  5.06031798  6.30461414\n",
      "   4.49264985  5.22496467  5.34550555  5.01300295  4.99815492  4.83109061\n",
      "   5.50643129  5.13627635]\n",
      " [ 5.04619891  4.90422542  4.97529869  4.82722483  4.21435533  5.23223575\n",
      "   5.55922928  4.19739906  4.24952963  4.73717833  4.72004395  4.49264985\n",
      "   6.14300991  5.31934972  5.38994519  5.24636125  4.9840392   4.53734858\n",
      "   5.66833852  5.30169727]\n",
      " [ 6.04905348  6.00659135  5.78626953  5.27617474  4.69014996  6.22647147\n",
      "   6.79290257  5.23296105  4.90897012  5.47801761  5.16829472  5.22496467\n",
      "   5.31934972  7.92159338  5.6891608   5.64515437  5.85037017  4.45106056\n",
      "   6.11710323  6.09139205]\n",
      " [ 5.28921179  5.60138633  5.22506549  5.15316218  5.14161442  5.5285925\n",
      "   5.91245522  5.09937038  4.60687165  4.92648508  5.03436515  5.34550555\n",
      "   5.38994519  5.6891608   7.18508558  5.36563971  5.19290589  5.20131662\n",
      "   5.74183904  6.08417579]\n",
      " [ 5.75538848  5.97912567  4.86697288  5.31955995  4.88927522  5.87090877\n",
      "   6.28401632  5.26179042  4.45203149  5.09645573  4.83030784  5.01300295\n",
      "   5.24636125  5.64515437  5.36563971  6.70025982  5.13992297  5.51257941\n",
      "   6.15280942  5.76114799]\n",
      " [ 5.34123359  5.1180608   5.08831112  4.86694351  4.1996268   5.30859608\n",
      "   5.63237327  4.61080017  4.45949958  4.7073776   4.78027388  4.99815492\n",
      "   4.9840392   5.85037017  5.19290589  5.13992297  6.37959131  4.41435708\n",
      "   5.42066408  5.8685674 ]\n",
      " [ 5.38001392  5.036689    4.38595062  5.10386795  4.41880525  5.32550337\n",
      "   5.57559039  5.47364828  4.52958746  4.45144078  4.58695315  4.83109061\n",
      "   4.53734858  4.45106056  5.20131662  5.51257941  4.41435708  6.67542603\n",
      "   5.42261429  5.32132017]\n",
      " [ 5.61558495  6.14664199  5.6744452   5.76716533  5.19617987  6.4972565\n",
      "   7.20765799  5.44274183  4.849667    6.09721564  5.71676095  5.50643129\n",
      "   5.66833852  6.11710323  5.74183904  6.15280942  5.42066408  5.42261429\n",
      "   8.35986858  6.36269955]\n",
      " [ 5.95419905  5.7425081   5.65699153  5.5800473   4.90755185  6.10561302\n",
      "   6.37142335  5.31884808  4.79341285  5.22444098  5.46454585  5.13627635\n",
      "   5.30169727  6.09139205  6.08417579  5.76114799  5.8685674   5.32132017\n",
      "   6.36269955  7.87264228]]\n",
      "\n",
      "The accuracy for LDA is 0.497080900751\n",
      "\n",
      "The precision for LDA is 0.4864\n",
      "\n",
      "The recall for LDA is 0.518771331058\n",
      "\n",
      "The F1 score for LDA is 0.502064409579\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for kNN with k value of 1 is 0.517097581318\n",
      "\n",
      "The precision for kNN with k value of 1 is 0.505546751189\n",
      "\n",
      "The recall for kNN with k value of 1 is 0.544368600683\n",
      "\n",
      "The F1 score for kNN with k value of 1 is 0.524239934265\n",
      "\n",
      "The accuracy for kNN with k value of 5 is 0.534612176814\n",
      "\n",
      "The precision for kNN with k value of 5 is 0.522292993631\n",
      "\n",
      "The recall for kNN with k value of 5 is 0.559726962457\n",
      "\n",
      "The F1 score for kNN with k value of 5 is 0.540362438221\n",
      "\n",
      "The accuracy for kNN with k value of 10 is 0.521267723103\n",
      "\n",
      "The precision for kNN with k value of 10 is 0.512195121951\n",
      "\n",
      "The recall for kNN with k value of 10 is 0.430034129693\n",
      "\n",
      "The F1 score for kNN with k value of 10 is 0.467532467532\n",
      "\n",
      "The accuracy for kNN with k value of 15 is 0.508757297748\n",
      "\n",
      "The precision for kNN with k value of 15 is 0.497622820919\n",
      "\n",
      "The recall for kNN with k value of 15 is 0.535836177474\n",
      "\n",
      "The F1 score for kNN with k value of 15 is 0.516023007395\n",
      "\n",
      "The accuracy for kNN with k value of 20 is 0.516263552961\n",
      "\n",
      "The precision for kNN with k value of 20 is 0.505976095618\n",
      "\n",
      "The recall for kNN with k value of 20 is 0.433447098976\n",
      "\n",
      "The F1 score for kNN with k value of 20 is 0.466911764706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "import math\n",
    "#Question 1\n",
    "#generate 2 classes with 20 features each\n",
    "#same covariance matrix\n",
    "#generate 2000 samples for each class\n",
    "#divide into train and test (70:30)\n",
    "#save DS1\n",
    "\n",
    "def data_generation(v, pos, size, data):\n",
    "    if v==0:\n",
    "        c=np.genfromtxt('hwk2_datasets_corrected/DS1_Cov.txt', delimiter=',')\n",
    "        if pos==False:\n",
    "            m=np.genfromtxt('hwk2_datasets_corrected/DS1_m_0.txt', delimiter=',')\n",
    "        else:\n",
    "            m=np.genfromtxt('hwk2_datasets_corrected/DS1_m_1.txt', delimiter=',')\n",
    "    else:\n",
    "        c=np.genfromtxt('hwk2_datasets_corrected/DS2_Cov' + str(v) + '.txt', delimiter=',')\n",
    "        if pos==False:\n",
    "            m=np.genfromtxt('hwk2_datasets_corrected/DS2_c1_m'+ str(v)+ '.txt', delimiter=',')\n",
    "        else:\n",
    "            m=np.genfromtxt('hwk2_datasets_corrected/DS2_c2_m'+ str(v)+ '.txt', delimiter=',')\n",
    "    cov=c[:, :-1]\n",
    "    mean=m[:-1]\n",
    "    for i in range(0, size-1):\n",
    "        sample=random.multivariate_normal(mean, cov)\n",
    "        if pos==True:\n",
    "            data.append((sample, 'pos'))\n",
    "        else:\n",
    "            data.append((sample, 'neg'))\n",
    "    return data\n",
    "\n",
    "def datasets(n):\n",
    "    if n==1:\n",
    "        pos_data= data_generation(0, True, 2001, [])\n",
    "        total_data=data_generation(0, False, 2001, pos_data)\n",
    "        DS1  = open('DS1.txt', 'w')\n",
    "        for element in total_data:\n",
    "            DS1.write(str(element) + '\\n')\n",
    "    else:\n",
    "        pos_data_m1=data_generation(1, True, int((2000)*0.1)+1, [])\n",
    "        neg_data_m1=data_generation(1, False, int((2000)*0.1)+1, pos_data_m1)\n",
    "        pos_data_m2=data_generation(2, True, int((2000)*0.42)+1, neg_data_m1)\n",
    "        neg_data_m2=data_generation(2, False, int((2000)*0.42)+1, pos_data_m2)\n",
    "        pos_data_m3=data_generation(3, True, int((2000)*0.48)+1, neg_data_m2)\n",
    "        total_data=data_generation(3, False, int((2000)*0.48)+1, pos_data_m3)\n",
    "        DS2  = open('DS2.txt', 'w')\n",
    "        for element in total_data:\n",
    "            DS2.write(str(element) + '\\n')\n",
    "    return total_data\n",
    "\n",
    "def train_test(dataset):\n",
    "    random.shuffle(dataset)\n",
    "    train = dataset[:int((len(dataset)+1)*.70)] #Remaining 70% to training set\n",
    "    test = dataset[int(len(dataset)*.70+1):] #Splits 30% data to test set\n",
    "    return train, test\n",
    "\n",
    "#Question 2\n",
    "#estimate the parameters of the probabilistic LDA model using mle\n",
    "#report accuracy, precision, recall, f-measure, coefficients learned\n",
    "\n",
    "#LDA - calculate the probabilities of both classes and choose whichever is higher\n",
    "\n",
    "def priors_means(data):\n",
    "    positive=0.0\n",
    "    negative=0.0\n",
    "    dimension=len(data[0][0])\n",
    "    zero=[]\n",
    "    positive_mean=np.pad(zero, (0, dimension), 'constant')\n",
    "    negative_mean=np.pad(zero, (0, dimension), 'constant')\n",
    "    for element in data:\n",
    "        if element[1]=='pos':\n",
    "            positive+=1\n",
    "            positive_mean=[i+j for i,j in zip(positive_mean, element[0])]\n",
    "        else:\n",
    "            negative+=1\n",
    "            negative_mean=[i+j for i,j in zip(negative_mean,element[0])]\n",
    "            \n",
    "    for i in range(0, len(positive_mean)):\n",
    "        positive_mean[i]=positive_mean[i]/positive\n",
    "    for i in range(0, len(negative_mean)):\n",
    "        negative_mean[i]=negative_mean[i]/negative\n",
    "    return positive/len(data), negative/len(data), np.asarray(positive_mean), np.asarray(negative_mean)\n",
    "\n",
    "\n",
    "def covariance_matrix(data, pos_mean, neg_mean):\n",
    "    dimension=len(data[0][0])\n",
    "    pos_sum=np.zeros((20, 20), dtype=float)\n",
    "    neg_sum=np.zeros((20, 20), dtype=float)\n",
    "    output=[]\n",
    "    for element in data:\n",
    "        if element[1]=='pos':\n",
    "            m=np.subtract(element[0], pos_mean)\n",
    "            matrix=m.reshape(-1,1)\n",
    "            matrix_transposed=m.reshape(1,-1)\n",
    "            multiplied=np.matmul(matrix, matrix_transposed)\n",
    "            for i in range (0, len(multiplied)):\n",
    "                for j in range (0, len(multiplied[0])):\n",
    "                    multiplied[i][j]=multiplied[i][j]/len(data)\n",
    "            pos_sum=np.add(pos_sum, multiplied)\n",
    "        else:\n",
    "            m=np.subtract(element[0], neg_mean)\n",
    "            matrix=m.reshape(-1,1)\n",
    "            matrix_transposed=m.reshape(1,-1)\n",
    "            multiplied=np.matmul(matrix, matrix_transposed)\n",
    "            for i in range (0, len(multiplied)):\n",
    "                for j in range (0, len(multiplied[0])):\n",
    "                    multiplied[i][j]=multiplied[i][j]/len(data)\n",
    "            neg_sum=np.add(neg_sum, multiplied)\n",
    "    return np.add(pos_sum, neg_sum)\n",
    "\n",
    "def calculate_decision(prior, mean, cov_matrix, x):\n",
    "    log_prior=math.log(prior)\n",
    "    mean_transposed=mean.reshape(1,-1)\n",
    "    mean_reshaped=mean.reshape(-1,1)\n",
    "    cov_inv=inv(cov_matrix)\n",
    "    term=(0.5)*np.matmul(np.matmul(mean_transposed, cov_inv), mean_reshaped)\n",
    "    x_transposed=x.reshape(1,-1)\n",
    "    last_term=np.matmul(np.matmul(x_transposed, cov_inv), mean_reshaped )\n",
    "    return log_prior-term+last_term\n",
    "\n",
    "def predict(pos_prior, neg_prior, pos_mean, neg_mean, cov_matrix, x):\n",
    "    pos_prediction=calculate_decision(pos_prior, pos_mean, cov_matrix, x)\n",
    "    neg_prediction=calculate_decision(neg_prior, neg_mean, cov_matrix, x)\n",
    "    if pos_prediction > neg_prediction:\n",
    "        return 'pos'\n",
    "    else:\n",
    "        return 'neg'\n",
    "    \n",
    "def LDA(train, test):\n",
    "    pos_prior, neg_prior, posmean, negmean=priors_means(train)\n",
    "    cov_matrix=covariance_matrix(train, posmean, negmean)\n",
    "    prediction=[]\n",
    "    for element in test:\n",
    "        prediction.append((element[0], predict(pos_prior, neg_prior, posmean, negmean, cov_matrix, element[0])))\n",
    "    print \"The coefficients for the LDA calculation are as follows: \\n\"\n",
    "    print \"The positive prior is \" + str(pos_prior) + \"\\n\"\n",
    "    print \"The negative prior is \" + str(neg_prior) + \"\\n\"\n",
    "    print \"The positive mean is \" + str(posmean) + \"\\n\"\n",
    "    print \"The negative mean is \" + str(negmean) + \"\\n\"\n",
    "    print \"The covariance matrix is \" + str(cov_matrix) + \"\\n\"\n",
    "    return prediction\n",
    "\n",
    "def accuracy(predicted, actual):\n",
    "    correct=0.0\n",
    "    for i in range (0, len(predicted)):\n",
    "        if predicted[i][1]==actual[i][1]:\n",
    "            correct+=1\n",
    "    return correct/len(predicted)\n",
    "\n",
    "def precision(predicted, actual):\n",
    "    true_positives=0.0\n",
    "    false_positives=0.0\n",
    "    for i in range (0, len(predicted)):\n",
    "        if predicted[i][1]=='pos':\n",
    "            if actual[i][1]=='pos':\n",
    "                true_positives+=1\n",
    "            else:\n",
    "                false_positives+=1\n",
    "    return true_positives/(true_positives+false_positives)\n",
    "\n",
    "def recall(predicted, actual):\n",
    "    true_positives=0.0\n",
    "    false_negatives=0.0\n",
    "    for i in range (0, len(predicted)):\n",
    "        if actual[i][1]=='pos':\n",
    "            if predicted[i][1]=='pos':\n",
    "                true_positives+=1\n",
    "            else:\n",
    "                false_negatives+=1\n",
    "    return true_positives/(true_positives+false_negatives)\n",
    "\n",
    "def f1(predicted, actual):\n",
    "    p=precision(predicted, actual)\n",
    "    r=recall(predicted, actual)\n",
    "    return 2*((p*r)/(p+r))\n",
    "    \n",
    "#run_tests(dataset)\n",
    "\n",
    "#find the k nearest training samples to x\n",
    "#y=majority(y1...yi)\n",
    "#let the distance metric be the square root of the sum of the square differences between points\n",
    "\n",
    "def distance(x1, x2):\n",
    "    sum_of_squares=[math.pow((i-j), 2) for i,j in zip(x1, x2)]\n",
    "    total_sum=0\n",
    "    for element in sum_of_squares:\n",
    "        total_sum+=element\n",
    "    return math.sqrt(total_sum)\n",
    "\n",
    "def kNN(train, test, k):\n",
    "    store=[]\n",
    "    output=[]\n",
    "    for element in test:\n",
    "        neighbors={}\n",
    "        for neighbor in train:\n",
    "            d=distance(element[0], neighbor[0])\n",
    "            neighbors[d]=neighbor\n",
    "        sorted_keys= sorted(neighbors.iterkeys())\n",
    "        to_add=[]\n",
    "        for key in sorted_keys[0:k]:\n",
    "            to_add.append(neighbors[key])\n",
    "        pos_count=0\n",
    "        neg_count=0\n",
    "        for n in to_add:\n",
    "            if n[1]=='pos':\n",
    "                pos_count+=1\n",
    "            else:\n",
    "                neg_count+=1\n",
    "        if pos_count>neg_count:\n",
    "            store.append((element[0], 'pos'))\n",
    "        else:\n",
    "            store.append((element[0], 'neg'))\n",
    "    return store\n",
    "\n",
    "\n",
    "def run_tests(data):\n",
    "    train, test=train_test(data)\n",
    "    predicted_LDA=LDA(train, test)\n",
    "    print \"The accuracy for LDA is \" + str(accuracy(predicted_LDA, test)) + \"\\n\"\n",
    "    print \"The precision for LDA is \" + str(precision(predicted_LDA, test))+ \"\\n\"\n",
    "    print \"The recall for LDA is \" + str(recall(predicted_LDA, test))+ \"\\n\"\n",
    "    print \"The F1 score for LDA is \"+ str(f1(predicted_LDA, test))+ \"\\n\"\n",
    "    \n",
    "    for element in [1, 5, 10, 15, 20]:\n",
    "        predicted_knn=kNN(train, test, element)\n",
    "        print \"The accuracy for kNN with k value of \" + str(element)+\" is \" + str(accuracy(predicted_knn, test))+ \"\\n\"\n",
    "        print \"The precision for kNN with k value of \" + str(element)+\" is \" + str(precision(predicted_knn, test))+ \"\\n\"\n",
    "        print \"The recall for kNN with k value of \" + str(element)+\" is \" + str(recall(predicted_knn, test))+ \"\\n\"\n",
    "        print \"The F1 score for kNN with k value of \" + str(element)+\" is \" + str(f1(predicted_knn, test))+ \"\\n\"\n",
    "\n",
    "    \n",
    "def main():\n",
    "    dataset1=datasets(1)\n",
    "    print \"Here are the results for DS1: \\n\"\n",
    "    run_tests(dataset1)\n",
    "    print \"\\n\"\n",
    "    print \"Here are the results for DS2: \\n\"\n",
    "    dataset2=datasets(2)\n",
    "    run_tests(dataset2)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
