{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 for the random classifier for train is 0.1851455543\n",
      "The F1 for the random classifier for valid is 0.189607003066\n",
      "The F1 for the random classifier for test is 0.182934662295\n",
      "The F1 for the majority classifier for train is 0.104267004647\n",
      "The F1 for the majority classifier for valid is 0.105014749263\n",
      "The F1 for the majority classifier for test is 0.103923019985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector has been produced\n",
      "Vector has been produced\n",
      "Vector has been produced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "The best parameters are {'alpha': 0.01, 'fit_prior': False}\n",
      "I've fit my model\n",
      "The F1 for train for naive is 0.773283909627\n",
      "The F1 for valid for naive is 0.386017630302\n",
      "The F1 for test for naive is 0.372590712913\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "The best parameters are {'max_features': 5000, 'min_samples_split': 2, 'splitter': 'best', 'max_depth': 500, 'min_samples_leaf': 1}\n",
      "I've fit my model\n",
      "The F1 for train for tree is 1.0\n",
      "The F1 for valid for tree is 0.298319101021\n",
      "The F1 for test for tree is 0.287980230973\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "The best parameters are {'kernel': 'linear', 'C': 1}\n",
      "I've fit my model\n",
      "The F1 for train for svm is 0.99518354146\n",
      "The F1 for valid for svm is 0.449159746111\n",
      "The F1 for test for svm is 0.448865121703\n",
      "The F1 for the random classifier for train is 0.174539751949\n",
      "The F1 for the random classifier for valid is 0.19658977349\n",
      "The F1 for the random classifier for test is 0.180241874897\n",
      "The F1 for the majority classifier for train is 0.104267004647\n",
      "The F1 for the majority classifier for valid is 0.105014749263\n",
      "The F1 for the majority classifier for test is 0.103923019985\n",
      "Vector has been produced\n",
      "Vector has been produced\n",
      "Vector has been produced\n",
      "I've fit my model\n",
      "The F1 for train for naive is 0.786481437661\n",
      "The F1 for valid for naive is 0.243085734095\n",
      "The F1 for test for naive is 0.255520713893\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "The best parameters are {'max_features': 1000, 'min_samples_split': 2, 'splitter': 'best', 'max_depth': 500, 'min_samples_leaf': 5}\n",
      "I've fit my model\n",
      "The F1 for train for tree is 0.663230936403\n",
      "The F1 for valid for tree is 0.268264330386\n",
      "The F1 for test for tree is 0.29633541547\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "The best parameters are {'kernel': 'linear', 'C': 100}\n",
      "I've fit my model\n",
      "The F1 for train for svm is 0.70236243902\n",
      "The F1 for valid for svm is 0.446687993514\n",
      "The F1 for test for svm is 0.46144492017\n",
      "The F1 for the random classifier for train is 0.505646918962\n",
      "The F1 for the random classifier for valid is 0.508777209349\n",
      "The F1 for the random classifier for test is 0.502204055462\n",
      "The F1 for the majority classifier for train is 0.666666666667\n",
      "The F1 for the majority classifier for valid is 0.666666666667\n",
      "The F1 for the majority classifier for test is 0.666666666667\n",
      "Vector has been produced\n",
      "Vector has been produced\n",
      "Vector has been produced\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "The best parameters are {'alpha': 0.03, 'fit_prior': True}\n",
      "I've fit my model\n",
      "The F1 for train for naive is 0.870795262267\n",
      "The F1 for valid for naive is 0.840794004456\n",
      "The F1 for test for naive is 0.83027881883\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "The best parameters are {'max_features': 5000, 'min_samples_split': 500, 'splitter': 'random', 'max_depth': 1000, 'min_samples_leaf': 10}\n",
      "I've fit my model\n",
      "The F1 for train for tree is 0.754941781749\n",
      "The F1 for valid for tree is 0.715768261965\n",
      "The F1 for test for tree is 0.724424112384\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'kernel': 'rbf', 'C': 50}\n",
      "I've fit my model\n",
      "The F1 for train for svm is 0.931373195333\n",
      "The F1 for valid for svm is 0.878193701723\n",
      "The F1 for test for svm is 0.874227294341\n",
      "The F1 for the random classifier for train is 0.500798722045\n",
      "The F1 for the random classifier for valid is 0.500798722045\n",
      "The F1 for the random classifier for test is 0.496231005464\n",
      "The F1 for the majority classifier for train is 0.666666666667\n",
      "The F1 for the majority classifier for valid is 0.666666666667\n",
      "The F1 for the majority classifier for test is 0.666666666667\n",
      "Vector has been produced\n",
      "Vector has been produced\n",
      "Vector has been produced\n",
      "I've fit my model\n",
      "The F1 for train for naive is 0.85661971831\n",
      "The F1 for valid for naive is 0.747273689783\n",
      "The F1 for test for naive is 0.663398984048\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "The best parameters are {'max_features': 5000, 'min_samples_split': 500, 'splitter': 'random', 'max_depth': 1000, 'min_samples_leaf': 10}\n",
      "I've fit my model\n",
      "The F1 for train for tree is 0.754409586643\n",
      "The F1 for valid for tree is 0.720700765203\n",
      "The F1 for test for tree is 0.726625111309\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n",
      "tuning...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import OrderedDict\n",
    "import codecs\n",
    "import random\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "def data_loader(v):\n",
    "    testf=data=open('hwk3_datasets/'+ v +'-test.txt', 'r')\n",
    "    trainf=data=open('hwk3_datasets/'+ v + '-train.txt', 'r')\n",
    "    validf=data=open('hwk3_datasets/'+ v + '-valid.txt', 'r')\n",
    "    replace_punctuation = string.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    train_tokens=trainf.read().lower().translate(None, string.punctuation).split(\"\\n\")\n",
    "    valid_tokens=validf.read().lower().translate(None, string.punctuation).split(\"\\n\")\n",
    "    test_tokens=testf.read().lower().translate(None, string.punctuation).split(\"\\n\")\n",
    "    return tokenize(train_tokens), tokenize(valid_tokens), tokenize(test_tokens)\n",
    "\n",
    "def tokenize(data):\n",
    "    output=[]\n",
    "    for element in data:\n",
    "        if len(element)==0:\n",
    "            pass\n",
    "        else:\n",
    "            tokens=word_tokenize(element)\n",
    "            review= tokens[0:-1]\n",
    "            review_score= tokens[-1]\n",
    "            output.append((review, int(review_score)))\n",
    "    return output\n",
    "\n",
    "def get_vocab(data):\n",
    "    vocab={}\n",
    "    count=0\n",
    "    for element in data:\n",
    "        for word in element[0]:\n",
    "            if word in vocab:\n",
    "                pass\n",
    "            else:\n",
    "                vocab[word]=count\n",
    "                count+=1\n",
    "    return vocab\n",
    "\n",
    "def get_frequency(data):\n",
    "    vocab={}\n",
    "    for element in data:\n",
    "        for word in element[0]:\n",
    "            if word in vocab:\n",
    "                vocab[word]+=1\n",
    "            else:\n",
    "                vocab[word]=1\n",
    "    sorted_vocab= sorted(vocab.iteritems(), key=lambda (k,v): (v,k), reverse=True)\n",
    "    return sorted_vocab[:10000]\n",
    "\n",
    "def get_feature_set(data):\n",
    "    word_frequencies=get_frequency(data)\n",
    "    feature_set=[]\n",
    "    for element in word_frequencies:\n",
    "        feature_set.append(element[0])\n",
    "    return feature_set\n",
    "        \n",
    "def generate_binary_feature_vector(data, feature_set):\n",
    "    vector=[]\n",
    "    for element in data:\n",
    "        row=[]\n",
    "        for word in feature_set:\n",
    "            if word in element[0]:\n",
    "                row.append(1)\n",
    "            else:\n",
    "                row.append(0)\n",
    "        vector.append(row)\n",
    "    return vector\n",
    "\n",
    "def binary_bag_of_words(data, feature_set):\n",
    "    vector=generate_binary_feature_vector(data, feature_set)\n",
    "    print \"Vector has been produced\"\n",
    "    return vector\n",
    "\n",
    "def generate_frequency_feature_vector(data, feature_set):\n",
    "    vector=[]\n",
    "    for element in data:\n",
    "        row=np.pad([], (0,10000), 'constant')\n",
    "        for word in element[0]:\n",
    "            if word in feature_set:\n",
    "                row[feature_set.index(word)]+=1\n",
    "            else:\n",
    "                pass\n",
    "        vector.append(row)\n",
    "    return vector\n",
    "\n",
    "def normalize(vector):\n",
    "    for row in vector:\n",
    "        total=0\n",
    "        for element in row:\n",
    "            total+=element\n",
    "        if total==0:\n",
    "            pass\n",
    "        else:\n",
    "            for i in range(0, len(row)):\n",
    "                row[i]=float(row[i])/total\n",
    "    return vector\n",
    "\n",
    "def frequency_bag_of_words(data, feature_set):\n",
    "    vector=generate_frequency_feature_vector(data, feature_set)\n",
    "    print \"Vector has been produced\"\n",
    "    return normalize(vector)\n",
    "\n",
    "def write_vocab(version, f):\n",
    "    to_write=codecs.open(f, 'w', 'utf-8')\n",
    "    train, valid, test=data_loader(version)\n",
    "    vocab=get_vocab(train)\n",
    "    frequency=get_frequency(train)\n",
    "    for element in frequency:\n",
    "        to_write.write(unicode(str(element[0]) + ' ' + str(vocab[element[0]])+ ' ' + str(element[1]) + '\\n', errors='ignore'))\n",
    "\n",
    "def datasets(data, f):\n",
    "    vocab=get_vocab(data)\n",
    "    for element in data:\n",
    "        for i in range (0, len(element[0])):\n",
    "            if i==len(element[0]):\n",
    "                f.write(str(vocab[element[0][i]]))\n",
    "            else:\n",
    "                f.write(str(vocab[element[0][i]]) + ' ')\n",
    "        f.write('\\t'+ str(element[1]) + '\\n')\n",
    "    \n",
    "def write_datasets(version, f1, f2, f3):\n",
    "    to_write1=codecs.open(f1, 'w', 'utf-8')\n",
    "    to_write2=codecs.open(f2, 'w', 'utf-8')\n",
    "    to_write3=codecs.open(f3, 'w', 'utf-8')\n",
    "    train, valid, test=data_loader(version)\n",
    "    datasets(train, to_write1)\n",
    "    datasets(valid, to_write2)\n",
    "    datasets(test, to_write3)\n",
    "\n",
    "def random_classifier(data, version, classes, avg):\n",
    "    prediction=[]\n",
    "    for element in data:\n",
    "        selection=random.choice(classes)\n",
    "        prediction.append(selection)\n",
    "    correct=get_y(data)\n",
    "    print \"The F1 for the random classifier for \" + version +\" is \" + str(f1_score(correct, prediction, average=avg))\n",
    "\n",
    "def majority_classifier(train_data, test_data, version, avg):\n",
    "    class_counts={}\n",
    "    for element in train_data:\n",
    "        if element[1] in class_counts:\n",
    "            class_counts[element[1]]+=1\n",
    "        else:\n",
    "            class_counts[element[1]]=1\n",
    "    sorted_classes=sorted(class_counts.iteritems(), key=lambda (k,v): (v,k), reverse=True)\n",
    "    majority=sorted_classes[0][0]\n",
    "    output=[]\n",
    "    for element in test_data:\n",
    "        output.append(majority)\n",
    "    correct=get_y(test_data)\n",
    "    print \"The F1 for the majority classifier for \" + version +\" is \" + str(f1_score(correct, output, average=avg))\n",
    "\n",
    "def get_y(data):\n",
    "    vector=[]\n",
    "    for element in data:\n",
    "        vector.append(element[1])\n",
    "    return vector\n",
    "\n",
    "def tune(x, y, valid_vector, valid_y, classifier, avg):\n",
    "    if classifier=='naive':\n",
    "        param_grid={'alpha': [0, 0.01, 0.03, 0.05, 0.07, 0.09, 0.1], 'fit_prior': [True, False]}\n",
    "        params=list(ParameterGrid(param_grid))\n",
    "        best_score=0.0\n",
    "        best_params=[[]]\n",
    "        for param in params:\n",
    "            clf = BernoulliNB(alpha=param['alpha'], fit_prior=param['fit_prior'])\n",
    "            clf.fit(x, y)\n",
    "            predicted_valid=clf.predict(valid_vector)\n",
    "            f1=f1_score(valid_y ,predicted_valid , average=avg)\n",
    "            if f1>best_score:\n",
    "                best_score=f1\n",
    "                best_params[0]=param\n",
    "            print \"tuning...\"\n",
    "    if classifier=='tree':\n",
    "        param_grid={'splitter': ['best', 'random'], 'max_depth': [1, 500, 1000], 'min_samples_split':[2, 500, 1000], 'min_samples_leaf':[1, 5, 10], 'max_features':[1000, 5000, 10000]}\n",
    "        params=list(ParameterGrid(param_grid))\n",
    "        best_score=0.0\n",
    "        best_params=[[]]\n",
    "        for param in params:\n",
    "            clf = tree.DecisionTreeClassifier(splitter=param['splitter'], max_depth=param['max_depth'], min_samples_split=param['min_samples_split'], min_samples_leaf=param['min_samples_leaf'], max_features=param['max_features'])\n",
    "            clf.fit(x, y)\n",
    "            predicted_valid=clf.predict(valid_vector)\n",
    "            f1=f1_score(valid_y ,predicted_valid , average=avg)\n",
    "            if f1>best_score:\n",
    "                best_score=f1\n",
    "                best_params[0]=param\n",
    "            print \"tuning...\"\n",
    "    if classifier=='svm':\n",
    "        param_grid={'C': [1, 50, 100], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "        params=list(ParameterGrid(param_grid))\n",
    "        best_score=0.0\n",
    "        best_params=[{}]\n",
    "        for param in params:\n",
    "            clf = svm.SVC(C=param['C'], kernel=param['kernel'])\n",
    "            clf.fit(x, y)\n",
    "            predicted_valid=clf.predict(valid_vector)\n",
    "            f1=f1_score(valid_y ,predicted_valid , average=avg)\n",
    "            if f1>best_score:\n",
    "                best_score=f1\n",
    "                best_params[0]=param\n",
    "            print \"tuning...\"\n",
    "    print \"The best parameters are \" + str(best_params[0])\n",
    "    return best_params[0]\n",
    "\n",
    "def predict(x, y, valid_vector, valid_y, test_vector, test_y, binary, model, avg):\n",
    "    if model=='naive':\n",
    "        if binary:\n",
    "            params=tune(x, y, valid_vector, valid_y, model, avg)\n",
    "            clf = BernoulliNB(alpha=params['alpha'], fit_prior=params['fit_prior'])\n",
    "        else:\n",
    "            clf = GaussianNB()\n",
    "    if model=='tree':\n",
    "        params=tune(x, y, valid_vector, valid_y, model, avg)\n",
    "        clf = tree.DecisionTreeClassifier(splitter=params['splitter'], max_depth=params['max_depth'], min_samples_split=params['min_samples_split'], min_samples_leaf=params['min_samples_leaf'], max_features=params['max_features'])\n",
    "    if model=='svm':\n",
    "        params=tune(x, y, valid_vector, valid_y, model, avg)\n",
    "        clf = svm.SVC(C=params['C'], kernel=params['kernel'])\n",
    "    clf.fit(x, y)\n",
    "    print \"I've fit my model\"\n",
    "    predicted_train=clf.predict(x)\n",
    "    print \"The F1 for train for \" + model+  \" is \" + str(f1_score(y, predicted_train, average=avg ))\n",
    "    predicted_valid=clf.predict(valid_vector)\n",
    "    print \"The F1 for valid for \" + model+  \" is \" + str(f1_score(valid_y ,predicted_valid , average=avg ))\n",
    "    predicted_test=clf.predict(test_vector)\n",
    "    print \"The F1 for test for \" + model+  \" is \" + str(f1_score(test_y, predicted_test, average=avg ))\n",
    "    \n",
    "def tester(train, valid, test, binary, avg):\n",
    "    output=[]\n",
    "    feature_set=get_feature_set(train)\n",
    "    if binary:\n",
    "        x=binary_bag_of_words(train, feature_set)\n",
    "        to_classify_valid=binary_bag_of_words(valid, feature_set)\n",
    "        to_classify_test=binary_bag_of_words(test, feature_set)\n",
    "    else:\n",
    "        x=frequency_bag_of_words(train, feature_set)\n",
    "        to_classify_valid=frequency_bag_of_words(valid, feature_set)\n",
    "        to_classify_test=frequency_bag_of_words(test, feature_set)\n",
    "    y=get_y(train)\n",
    "    correct_y_valid=get_y(valid)\n",
    "    correct_y_test=get_y(test)\n",
    "    predict(x, y, to_classify_valid, correct_y_valid, to_classify_test, correct_y_test, binary, 'naive', avg)\n",
    "    predict(x, y, to_classify_valid, correct_y_valid, to_classify_test, correct_y_test, binary, 'tree', avg)\n",
    "    predict(x, y, to_classify_valid, correct_y_valid, to_classify_test, correct_y_test, binary, 'svm', avg)\n",
    "def run_tests(version, train, valid, test, binary, classes, avg):\n",
    "    random_classifier(train, 'train', classes, avg)\n",
    "    random_classifier(valid, 'valid', classes, avg)\n",
    "    random_classifier(test, 'test', classes, avg)\n",
    "    \n",
    "    majority_classifier(train, train, 'train', avg)\n",
    "    majority_classifier(train, valid, 'valid', avg)\n",
    "    majority_classifier(train, test, 'test', avg)\n",
    "    \n",
    "    tester(train, valid, test, binary, avg)\n",
    "\n",
    "def tests(version, binary):\n",
    "    train, valid, test=data_loader(version)\n",
    "    if version=='yelp':\n",
    "        run_tests('yelp', train, valid, test, binary, [1, 2, 3, 4, 5], 'macro')\n",
    "    else:\n",
    "        run_tests('IMDB', train, valid, test, binary, [0,1], 'binary')\n",
    "\n",
    "def main():\n",
    "    tests('yelp', True)\n",
    "    tests('yelp', False)\n",
    "    tests('IMDB', True)\n",
    "    tests('IMDB', False)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
